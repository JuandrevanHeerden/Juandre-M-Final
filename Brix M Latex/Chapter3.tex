\chapter{Overview of Machine Learning Technologies}
\label{chap: Chapter 3}
The development in machine learning and information retrieval in recent years has resulted in a surge in recommender systems. However, machine learning and information retrieval plays an integral part in the success and failure of a recommender system. This chapter  will cover three popular types of learning: (1) reinforcement learning, (2) supervised learning, and (3) unsupervised learning. This is followed by document cluster, how it works, and challenges pertaining to document clustering. This chapter will also investigate the different problems that natural language processing faces and what tasks are involved.
Lastly, topic modelling will be discussed and how it is used.

\section{Machine Learning} \label{ssec:MLoverview}


Machine learning (ML) is a subfield of artificial intelligence that is concerned with building algorithms which rely on a collection of some phenomenon \cite{Andriybook2019}. These examples can come from nature, can be created by developers, or can be generated by other ML algorithms. Machine learning is also known as attempting to solve problems by: (1) acquiring a dataset, and (2) automatically building a model using that dataset \cite{sebastiani2002machine}. 

The rise of big data in recent years has created the problem of how to translate untouchable data into knowledge. Technological advances have enabled machine learning to solve these problems. Machine learning plays an integral part in the following sectors \cite{alpaydin2009introduction}:

\begin{itemize}
  \item Computational finance – in the credit section of banks, for credit scoring.
  \item Computer vision – for face recognition, object detection, and motion detection.
  \item Natural language processing – for text analysis and text summarisation.
\end{itemize}

Games or simulations were not mentioned in the list above. Simulations bring forth a new need that has to be satisfied. Furthermore, simulation brings a decision-making aspect.

\subsection{Reinforcement Learning}

Reinforcement learning (RL) can be defined as a machine, capable of getting the state of an environment as input also known as features \cite{DBLP:journals/corr/abs-1806-08894}. Actions are executed within each state. Furthermore, each action can give rewards, which can move the machine to another state. As mentioned, reinforcement learning works on a reward system and each component, as indicated in Figure \ref{fig:RLreward} will be discussed below.


\begin{figure}[htbp]
\centering
\includegraphics[width=6cm]{./figures/reinforce7.png}
\caption{Reinforcement Learning reward system \protect\cite{sebastiani2002machine}}
\label{fig:RLreward}
\end{figure}
\begin{itemize}
  \item Autonomous agent - it is the  responsibility of the agent to take action.
  \item Actions - it can be seen as a set of logical steps which are needed to move forward. An action can occur in two states; either reward or penalty.
  \item Environment - this is the environment in which the autonomous agent finds itself.
  \item Reward - the main aim of reinforcement learning is to obtain rewards, either good or bad.
  \item State - this is defined as the position in which it finds itself in the environment. To move around in the environment, the state needs to keep changing.
\end{itemize}

The goal of a reinforcement learning algorithm is to learn a near-optimal policy that thrives on the reward system. A policy is the rules of the game. Reinforcement learning focuses on addressing problems, which include decision-making \cite{Andriybook2019}. This works well for games, robotics, and logistics.

\subsection{Supervised Learning}

Supervised learning (SL) can be defined as a task that is learning the mapping between input and output by looking at examples of the input and output pairs. It creates a model from the labelled training data consisting of a set of training examples. Labelled suggests that the mapping between questions and answers, or between input and output, has already been done \cite{singh2019natural}. For example, take a financial company that wants to look at users’ profiles to decide whether to give them a loan or not. A machine learning model would be trained on historical labelled data, which consists of information regarding the profiles of past customers \cite{kotsiantis2007supervised}. 

The methodology that is used in supervised learning can sometimes vary based on the output of the model. Some supervised machine learning algorithms are listed here:

\begin{itemize}
  \item Logistic regression
  \item Decision trees
  \item Linear regression
  \item Support vector machines
\end{itemize}

Another integral part of supervised learning is evaluating the model. Based on the type of model,  the appropriate evaluation metric can be chosen and applied \cite{sebastiani2002machine}. Furthermore, this can be done by splitting the training data into two sets: train set, and validation set. Training of the model would be done on the training set and testing the performance should be done on the labelled validation set. Changes can be made in the hyperparameters to improve the performance of the model \cite{kotsiantis2007supervised}. Hyperparameters are used to configure various aspects of the learning algorithm and they  have a direct impact on the results and the performance of the created model.

\subsection{Unsupervised Learning}

In unsupervised learning, a model is trained on similar unlabelled data. Since the data does not contain labels and is sometimes unstructured, the model will just be trained without any influence given by labels. In unsupervised learning, the machine tries to find latent patterns and insights into the data that can be used in any form. The relevant unsupervised learning algorithms are:

\begin{itemize}
\item Clustering algorithms (document and hierarchical)
\item Dimensionality reduction techniques
\item Topic modeling
\end{itemize}

The rest of Chapter \ref{chap: Chapter 3} will cover different applications of unsupervised learning, and how they interact with one another.

\section{Document Clustering}

Clustering is the answer to the problem of learning to map a label to examples based on an unlabelled dataset. Owing to the dataset being unlabelled, one must decide whether the learned model is optimal, which makes it much more complicated than supervised learning. Clustering has several use cases ranging from text analysis to anomaly detection. A common use case for businesses is to use machine learning-driven clustering for profiling customers based on their activities and building strategies around these results.

Search engines have been employing clustering by finding similar searches and results in one cluster. Document clustering, a branch of clustering, is the technique that data mining uses, which includes concepts from fields of machine learning, information retrieval and natural language processing. Document clustering organises documents into different groups called clusters, where the documents in the cluster share some common features according to the similarity measure. Clustering, in general, can produce overlapping clusters or non-overlapping clusters. In an overlapping cluster, it is likely that multiple clusters can contain the same document \cite{andrews2007recent}; in a non-overlapping cluster, the opposite can happen.

An example of supervised learning and unsupervised learning, in terms of document clustering, can be in document classification, where all the classes and their properties are known beforehand. In document clustering, all the properties or other information are unknown. Thus, classification is an example of supervised learning, and clustering is an example of unsupervised learning \cite{andrews2007recent}.
Document clustering can be broken down into two sections: hard clustering and soft clustering \cite{chen2010integration}. Furthermore, soft clustering can be broken down even more, into partitioning and hierarchical.

\begin{itemize}
\item Hard clustering - this clusters the features to exactly one cluster.
\item Soft clustering - clusters feature into multiple clusters. For example, if the titles of papers need to be clustered, ‘natural language and information retrieval’ would be clustered in both the cluster names ‘natural language processing’ and ‘information retrieval’.
\begin{itemize}
    \item Partitioning - this type of clustering splits the documents into a fixed number of clusters. An example is k-means clustering \cite{chen2010integration}.
    \item Hierarchical - this is commonly known as taking shape as a tree of clusters.
  \end{itemize}
\end{itemize}

In this section, we discussed what document clustering is, covering the different types of clustering. In the next section, the applications of document clustering will be discussed.

\subsection{Document Clustering Applications}

As mentioned above, document clustering falls within supervised learning and can be used in various fields like business and science \cite{jain2010data}. The origin of document clustering research was to improve recall or precision in information retrieval (IR) systems; however, recently the application of document clustering has evolved drastically. The rate of development in technology has spiked research on how to improve document clustering \cite{alhawarat2018revisiting,mekonnen2017topic}. 

Document clustering can be applied  in different ways \cite{jajoo2008document,abualigah2017text}: First, it is a system which enables people to find documents that are similar to that which was inputted. Using document clustering enables systems to find other documents that are similar semantically \cite{Shah2013}. Second, it is useful to organise large numbers of documents into a taxonomic structure. Third, the number of documents in the information ocean created a need to find duplicates. 

Clustering use cases includes plagiarism detection, identifying related news stories and fake news, and optimises search engines \cite{jin2016news}. Lastly, in the most basic form, papers are recommended for an academic based on the papers they have already read. This can be done by using clustering and employing other features of the text, ultimately improving the quality of recommendations. The use of latent Dirichlet allocation and content-based filtering is evidence that it can work well, as depicted in \citeA{yeh2010}.

\subsection{Document Clustering Procedure}

Moving from a collection of documents to a cluster of documents a few processes need to be followed. The processes generally comprise three components: (1) feature extraction and selection, (2) document representation and, (3) document clustering \cite{shah2012document}.

Feature extraction takes the document and applies pre-processing steps to it. Cleaning the text includes the removal of stop words, which should be updated with the domain’s most common keywords, which would not add any value from a semantic perspective. The document should then be analysed and features should be extracted \cite{mugunthadevi2011survey}. Then, looking at the extracted features, selecting the right ones would be an important exercise to remove further noise. The benefit is that dimensionality is also reduced through only selecting the wanted features; it helps by enabling better data understanding \cite{wei2006combining}.

After the document has been stripped of all the unnecessary features, the documents are then left with only the features which scored the highest in the metric score \cite{Shah2013}. Term frequency (TF) would be an example of feature selection metrics. The documents are then grouped into clusters based on their features and the metric scores which were calculated \cite{wei2006combining}.

\subsubsection{Term Frequency-Inverse Document Frequency (TF-IDF)}

The dataset that will be clustered will be presented as a set of vectors of an object. Vector space model (VSM) is a model that represents text documents as vectors \cite{clark2015vector}.

Term weight value can be defined as the noteworthiness of a specific term in a document. This can be calculated by the number of times the term occurs within the document over the entire dataset. Term frequency with inverse document frequency (TF-IDF) is the most commonly used term weight scheme \cite{cui2005document}. More frequent the words in a document the more important \cite{peng2006recent}.

\subsubsection{Dimension Reduction}

The increase in vast amounts of data has highlighted the inefficiency of most dimension reduction algorithms \cite{mugunthadevi2011survey}. These algorithms are used for feature extraction and feature selection. While feature extraction is taking place, new features are combined with the original features, which causes the computation load to increase. In contrast to feature extraction, feature selection selects the features directly

\subsection{Similarity Measures for Document Clustering} \label{JSD}

Cluster similarity is  based on the measurements between objects. Three main steps are involved to determine the similarity between objects: (1) Identifiers need to be used to characterise the objects; (2) a weighting scheme needs to be selected; and (3) a similarity coefficient needs to be selected to determine the degree of resemblance between two vectors \cite{willett1988recent}.

Cluster accuracy is the precise distance between a pair of objects must be known  in terms of either similarity or distance. Distance and similarity measures have been proposed and are used widely, for example  (1) Cosine similarity, (2) Jaccard correlation coefficient, (3) Euclidean distance, and (4) relative entropy \cite{huang2008similarity}. An overview of the similarity measures are discussed in \citeA{huang2008similarity}.

\begin{itemize}
    \item Euclidean distance - this is a standard metric for geometrical problems. It is the distance between two points and is the default distance measure used in the k-means algorithm.
    \item Cosine similarity - the similarity of two documents corresponds to the correlation between the vectors.
    \item Jaccard coefficient - the Jaccard coefficient contrasts the sum weight of shared terms with the sum weight of terms that are present in either of the two documents but not the shared terms.
    \item Pearson correlation coefficient - this is an alternative to measure two vectors.
    \item Kullback–Leibler divergence - this can be used for evaluating the differences between two probability distributions.
    \item •	Jensen–Shannon divergence - this is based on the Kullback–Leibler divergence with improved differences, for example, it always returns a finite value and is also symmetric.
\end{itemize}

Keeping above overview in mind, the Jensen–Shannon divergence has been reported to make a positive contribution when comparing two probability distributions \cite{Uto2017,9358561}.

\subsection{Summary}

Document clustering is a crucial and fundamental pillar in unsupervised document organisation. In this section, we have discussed what document clustering is, the types of clustering used, and the applications that employ document clustering. We also discussed the three phases needed to cluster documents, emphasising the importance of extracting and selecting the correct features, which later translate in the quality of clusters. The size of data sets has piqued the interest of researchers to look for alternative ways to reduce the dimensions of an SVM. Furthermore, we have highlighted the types of similarity measures that can be used. In the next section, natural language processing (NLP) will be discussed.

\section{Natural Language Processing} \label{secc:LDAover}

Natural language processing (NLP) is a sub-discipline of artificial intelligence and linguistics. The goal of natural language processing was to ease the user’s work and to communicate effectively with a computer \cite{khurana2017natural}. Since some users are not proficient in machine programming languages, NLP alleviates the pressure that time or lacking the ability to perfect machine language has on a person \cite{russell2016artificial}.

A language can be defined as a set of rules or a set of symbols \cite{santana2016language}. The symbols can be combined and used to convey information in a clear and concise manner. This being said, NLP can be classified into two sections: (1) Natural language understanding; and (2) Natural language
generation, which means to understand and to generate text as seen in Figure
\ref{fig:CNLP}.

\begin{figure}[htbp]
\centering
\includegraphics[width=10cm]{./figures/NLP3.eps}
\caption{Classification of NLP}
\label{fig:CNLP}
\end{figure}

Linguistics is the domain where languages are studied, which involves
the meaning of language, and the context in which language finds itself \cite{Bates9977}. The important terminologies of NLP are: (1) Phonology, that refers to the relationship in sound, (2) Morphology, which refers to word formation, (3) Syntax, which is the sentence structure, (4) Semantics, which refers to the arrangement of words and their meaning \cite{hassan2021natural} and (5) Pragmatics, which refers to understanding.

To simplify the complexity of natural language processing, it can be broken up into four distinct stages (See Figure \ref{fig:stepsnlp}). In a real-world scenario, these stages seldom occur separately. In the overview that follows, it is assumed that the syntactic analysis and semantic analysis is done by the pre-processing.
The rest of this section contains the processes shown in Figure \ref{fig:stepsnlp}.

\begin{figure}[htbp]
\centering
\includegraphics[width=10cm]{./figures/NLPstep.eps}
\caption{Steps in Natural Language Processing}
\label{fig:stepsnlp}
\end{figure}
\subsection{Morphological processing}

The first logical step in a typical NLP system is morphological processing. In this step the text will be broken down into sets of tokens corresponding to the equivalent words, sub-words, and punctuation forms \cite{Bates9977}. For example, a word like ‘unnecessarily’ can be broken down into three sub-word tokens: un - necessari - ly.

Morphology can be defined as a study of how words can be modified to have similar meanings but used in different syntactical ways. Modifying these words is typically done by adding prefixes or suffixes. Generally, word modification can be broken down into three components:

\begin{itemize}
    \item Inflection - words can be represented differently based on the syntax in which they find themselves.
    \item Derivation - new words are made from existing words. ‘Determines’, ‘determining’, and ‘determined’ all come from the root ‘determine’.
    \item Compounding: new words are made through the grouping of existing words. It is not used so much in English (for example, ‘toothpaste’) but is widely used in other languages.
\end{itemize}

Outputs from the morphology phase is a set of tokens. These tokens can contain identifiable data that is needed for the parser to do its job. The next stage of processing is syntax and semantic analysis.

\subsection{Syntax and Semantic analysis}

A language processor has certain tasks that it needs to perform: that is, syntax analysis and semantic analysis. There are two main aims for syntax analysis: (1) to check whether a sentence is well formed, and (2) to break up the structure to show syntactic relationships between the words. A syntactic analyser (parser) does this by using a dictionary of words (lexicon) and a set of syntax rules (grammar). The usage of a dictionary and syntax rules indicates how syntactic categories can be combined to form phrases of different types \cite{nation2007dissecting,liddy2001natural,feldman1999nlp}.

This syntax–semantic combination could deconstruct the sentence ‘The large cat chased the rat’ as follows:

\begin{figure}[htbp]
\centering
\includegraphics[width=10cm]{./figures/sen.eps}
\caption{Deconstruction of a sentence}
\label{fig:sentence}
\end{figure}

One of the tasks of a language processor is to analyse a sentence and to produce a formal notation that expresses the semantics of a sentence concisely; it is called semantic analysis.
When constructing a model, semantic analysis plays the role of finding the meaning of the words in the sentence. In order for that to happen, the dictionary of the model should include whether the words are nouns, verbs
or adjectives. The grammar rule in Figure \ref{fig:sentence} with VerbPhrase to Verb, NounPhrase states how the syntactic group is formed.

The key part of this section is understanding that the syntax and semantic analysis phase is of vital importance for any NLP system or tool.
The next stage of processing is semantics and pragmatics.

\subsection{Semantics and Pragmatic}

After the combined stages, syntax and semantic analysis, the next stage of processing is pragmatics. There is no clear distinction between semantics and pragmatics \cite{Stern2004}, but for the purpose of this study we make the distinction as follows: semantics studies the meaning of the word and their meaning within sentences, whereas pragmatics studies the same word and meaning but within a certain context. Doing semantic analysis on a sentence like ‘The large cat chased the rat’ can only provide a string of text which translates to the large cat (the identity of the cat). Pragmatic analysis, like the example supplied, simply maps the actual objects, which exist in a certain context, to a reference obtained during semantic analysis \cite{russell2016artificial}.

This section has provided examples of how analysing human languages creates certain challenges within the natural language processing domain.
In the next section, we will discuss topic modelling.

\section{Topic modeling} \label{ssec:tmodel}

Language models have recently been used to aid speech recognition and handwriting recognition by showing textual data. Language modelling can be defined as a probability distribution derived from words in an indexed vocabulary \cite{croft2010search}. 
A traditional generative model of a language can be used either to recognise patterns of strings or to generate documents \cite{Sajjadi2018AssessingGM}. 
The generative model, as illustrated in Figure \ref{fig:automon}, is a finite automaton that generates documents. 

\begin{figure}[htbp]
\centering
\includegraphics[width=7cm]{./figures/automon8.png}
\caption{A finite automaton and string it generates}
\label{fig:automon}
\end{figure}

Topic modelling has been used as a technique to identify concepts and annotate large text corpora, to keep track of topics over time, and to assess the similarity between topics and documents. The purpose of topic modelling is to analyse data or documents to look for patterns and latent topics. After the topics were identified they would be represented by means of a probability distribution. Topic modelling has been applied actively to several tasks, for the analysis of scientific patterns \cite{lau2012line,yi2009comparative,wei2006lda,yi2009comparative} and in scholarly publication search engines \cite{newman2010evaluating}.

Topic modelling refers to a range of generative models for language that specifies procedures by which documents are built \cite{blei2003latent}. The most preferred algorithm of topic modelling is latent Dirichlet allocation (LDA), which describes a generative model for topics and documents \cite{blei2003latent}.

\subsection{Latent Dirichlet Allocation Algorithm} \label{ssec:LDAA}

Latent Dirichlet allocation (LDA) is a type of topic model that associates multiple topics of a document. A topic is a distribution over a fixed vocabulary \cite{chaney2012visualizing}. In each topic the distribution of words is different, assuming the topics are specified before the documents are generated. The documents are generated through the following processes. Initially, the random distribution of topics is selected. For each word in the document, a random topic is selected from the distribution of topics. Finally, a word is selected from the topic \cite{blei2003latent,mekonnen2017topic}.

The goal of topic modelling is to discover topics from a collection of documents automatically. To compute the hidden topic structure from documents, the probability distribution of the hidden variables given must be computed \cite{mimno2012sparse}.

\subsection{Topic Model Validation}

The quality, performance, and efficiency of the topic model must be evaluated \cite{ramirez2012topic}. Topic validations have been created to compare the quality of different algorithms. The first approach is to evaluate the topic models based on perplexity, which is calculated on how well the topics were extracted using the training set and allows the prediction of the occurrence of words belonging to the training set \cite{ramirez2012topic}.

Other approaches focus on the semantic coherence of the topics. \citeA{chang2009reading} introduced human validation of topical coherence via intrusion tests. The judges had to find the intruder in the evaluated topics and if the intruder was easily detected that means the other words have a strong thematic correlation. However, the process requires manual validation of every built model. Automatic approaches have been proposed by \citeA{newman2010automatic}, using point-wise mutual information (PMI) to calculate the co-occurrence in Google search results for all given word pairs in the topic. This approach achieved similar results as human judges \cite{alsumait2009topic}.

\subsection{Additional Topic Modeling Tools and Techniques}

Many researchers have worked on NLP, building tools and systems which made NLP what it is today. There are tools such as sentiment analysers, part- of-speech (POS) taggers, Chunking, name entity recognition, emotion detection, and topic modelling. A sentiment analyser works by extracting sentiments about a given topic. Sentiment analysis consists of a topic feature extraction, sentiment extraction, and association by relationship analysis \cite{yi2003sentiment}. Sentiment analysis uses two linguistic resources, namely the sentiment lexicon and the sentiment pattern database \cite{nasukawa2003sentiment}. It analyses a document for positive and negative words and gives them a rating on a scale of -5 to +5.

A part-of-speech (POS) tagger can be defined as a piece of software that reads in text and assigns parts of speech to tokens; parts of speech like noun, verb and adjective. POS tagging is a daunting task because a word can represent more than one part of speech at different times. A substantial amount of research has been conducted in European languages, and research has shifted to improve POS taggers for other languages like Arabic, Sanskrit \cite{tapaswi2012treebank}, Hindi \cite{ranjan2003part}, etc. It can tag and classify words effectively as nouns, verbs, adjectives, etc. Technological improvements for part-of-speech tagging can work efficiently on European languages but still lacks advancement on Asian languages \cite{hirschberg2015advances}. The POS tagger used for the Sanskrit language uses the treebank technique \cite{bengoetxea2010application}. Arabic uses the Support Vector Machine (SVM) \cite{diab2004automatic} approach to tokenise, POS tag, and annotate phrases in Arabic text.

Chunking is also known as shadow parsing, it works by labeling pieces of a sentence with syntactically correlated keywords like noun phrase and verb phrase(NP or VP). Each sentence that is being tagged starts with a unique tag marked as begin chunk (B-NP) tag or inside chunk (I-NP) tag. Chunking can be evaluated by means of the CoNLL 2000 shared task, which provides test data for chunking \cite{sang2000introduction}. 

The usage of named entity recognition (NER) in places such as the internet is problematic because people do not use traditional or academic English \cite{nadeau2007survey}. This brings down the overall performance and quality of language processing tools. By annotating the phrases unlabelled, in-domain and out-of-domain data improves the performance compared to traditional language processing tools \cite{katiyar-cardie-2018-nested}.

Emotion detection is similar to sentiment analysis but is used in the social media scene in the mixing of two languages (English and one other language). It categorises statements into six different groups based on emotions, namely sadness, happiness, disgust, fear, surprise, and anger. During the categorising process, the identification of ambiguous words that are in English and the other language should commence by determining the base language of the text. Determining the base language would accelerate the performance and quality of the detection \cite{khurana2017natural}.

Event discovery in social media feeds use a graphical model and NER to determine whether it contains the name of a person, city, place, etc. The model operates by listening to noisy data and extracting records and keywords of the events from multiple data streams. Despite the noise and the use of irregular language, the model is able to extract records with very high accuracy \cite{benson2011event}.

\section{Summary}

There has been so much achieved in trying to satisfy the problem of information overload over the past few years; hence, machine learning has been used.
In this chapter we discussed why machine learning has played such a pivotal role in the development of these concepts: document clustering, natural language processing, and topic modelling.

The next chapter presents the research methodology that was followed.
